{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install evaluate peft==0.8.2 accelerate bitsandbytes -q"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re, regex, os, sys, warnings, random, gc, logging\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import train_test_split\n","\n","import evaluate\n","from datasets import Dataset\n","\n","import transformers\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    TrainingArguments,\n","    Trainer\n",")\n","from peft import (\n","    prepare_model_for_int8_training,\n","    LoraConfig,\n","    TaskType,\n","    get_peft_model\n",")\n","\n","SEED = 42\n","transformers.set_seed(SEED)\n","warnings.filterwarnings('ignore')\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","logging.getLogger().setLevel(logging.WARNING)\n","\n","INPUT_PATH = '/kaggle/input/translit-datasets/'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","from huggingface_hub import login\n","\n","login(UserSecretsClient().get_secret(\"hf\")) # for gemma-2b"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EPOCHS = 5"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{},"source":["## Transliterated Hindi Sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Insert appropriate dataset reading code from dataset_readers.py\n","\n","def read_dataset():\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df, test_df, label_names, dataset_name, text_col = read_dataset()\n","\n","train_df.label = train_df.label.cat.codes\n","test_df.label = test_df.label.cat.codes\n","\n","class_weights = dict(enumerate(\n","    compute_class_weight(\n","        class_weight=\"balanced\", \n","        classes=np.unique(train_df['label']), \n","        y=train_df['label']\n","    )\n","))\n","\n","pd.set_option('max_colwidth', 200)\n","display(train_df.head())\n","display(test_df.head())\n","\n","print(f'{len(train_df)=}, {len(test_df)=}')\n","print(label_names)\n","\n","plt.figure(figsize=(6,2))\n","plt.bar(x=label_names, height=np.bincount(train_df['label']))"]},{"cell_type":"markdown","metadata":{},"source":["# Gemma-2B"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model( \n","    model_name,\n","    train_args: dict,\n","    seed: int = SEED,\n","    train_df: pd.DataFrame = train_df, \n","    test_df: pd.DataFrame = test_df, \n","    label_names: list[str] = label_names,\n","    save_model: bool = True\n","):\n","    # Setup\n","    transformers.set_seed(seed)\n","    n_labels = len(label_names)\n","    id2label = {i:name for i, name in enumerate(label_names)}\n","    label2id = {name:i for i, name in enumerate(label_names)}\n","      \n","    ## Evaluation Metric\n","    metric = evaluate.load(\"f1\")\n","    def compute_metrics(eval_pred):\n","        logits, labels = eval_pred\n","        predictions = np.argmax(logits, axis=1)\n","        return metric.compute(predictions=predictions, references=labels, average='weighted')  \n","    \n","    ## Get LoRA model\n","    model = prepare_model_for_int8_training(\n","        AutoModelForSequenceClassification.from_pretrained(\n","            model_name, num_labels=n_labels, id2label=id2label, label2id=label2id, load_in_8bit=True\n","        )\n","    )\n","    model = (model)\n","    lora_model = get_peft_model(\n","        model, \n","        LoraConfig(\n","            r=64,\n","            lora_alpha=32,\n","            lora_dropout=0.1,\n","            task_type=TaskType.SEQ_CLS,\n","            target_modules='all-linear'\n","        )\n","    )\n","    \n","    # Make Dataset and tokenize    \n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    def preprocess_function(example):\n","        return tokenizer(example[\"sentence\"], truncation=True)\n"," \n","    train_df.sentence = train_df.sentence.apply(lambda text: text.lower())\n","    test_df.sentence  =  test_df.sentence.apply(lambda text: text.lower())\n","    \n","    tokenized_train = Dataset.from_pandas(train_df, split='train').map(preprocess_function, batched=True)\n","    tokenized_test  = Dataset.from_pandas( test_df, split='test' ).map(preprocess_function, batched=True)\n","        \n","    # Train and evaluate\n","    trainer = Trainer(\n","        model = lora_model,\n","        tokenizer = tokenizer,\n","        train_dataset = tokenized_train, \n","        eval_dataset = tokenized_test,\n","        compute_metrics = compute_metrics,\n","        data_collator = DataCollatorWithPadding(tokenizer=tokenizer),\n","        args = TrainingArguments(\n","            output_dir = './checkpoints/',\n","            report_to = 'none',\n","            evaluation_strategy = \"epoch\",\n","            save_strategy = \"epoch\",\n","            save_total_limit = 1,\n","            load_best_model_at_end = True,\n","            **train_args\n","        )\n","    )\n","    \n","    display(trainer.evaluate())\n","    train_output = trainer.train()\n","    display(trainer.evaluate())\n","    \n","    predictions  = trainer.predict(tokenized_test)\n","    \n","    y_test, y_pred = test_df.label, predictions.predictions.argmax(1)\n","    \n","    clf_report  = classification_report(y_test, y_pred, target_names=label_names, digits=5)\n","    conf_matrix = confusion_matrix(y_test, y_pred)\n","    \n","    if save_model:\n","        trainer.save_model(f\"/kaggle/working/model_{model_name.replace('/', '-')}/\")\n","        \n","    !rm -r './checkpoints/'\n","    return clf_report, conf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    'google/gemma-2b',\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 2e-5,\n","        'weight_decay': 0.01,\n","        'per_device_train_batch_size': 4,\n","        'logging_steps': 10,\n","    },\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["# TinyLLaMa"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model( \n","    model_name,\n","    train_args: dict,\n","    seed: int = SEED,\n","    train_df: pd.DataFrame = train_df, \n","    test_df: pd.DataFrame = test_df, \n","    label_names: list[str] = label_names,\n","    save_model: bool = True\n","):\n","    # Setup\n","    transformers.set_seed(seed)\n","    n_labels = len(label_names)\n","    id2label = {i:name for i, name in enumerate(label_names)}\n","    label2id = {name:i for i, name in enumerate(label_names)}\n","      \n","    ## Evaluation Metric\n","    metric = evaluate.load(\"f1\")\n","    def compute_metrics(eval_pred):\n","        logits, labels = eval_pred\n","        predictions = np.argmax(logits, axis=1)\n","        return metric.compute(predictions=predictions, references=labels, average='weighted')  \n","    \n","    ## Get model\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_name, num_labels=n_labels, id2label=id2label, label2id=label2id, load_in_8bit=True\n","    )\n","    \n","    # Make Dataset and tokenize    \n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    \n","    tokenizer.pad_token = tokenizer.eos_token  # llama only\n","    model.config.pad_token_id = model.config.eos_token_id\n","        \n","    def preprocess_function(example):\n","        return tokenizer(example[\"sentence\"], truncation=True)\n","    \n","    train_df.sentence = train_df.sentence.apply(lambda text: text.lower())\n","    test_df.sentence  =  test_df.sentence.apply(lambda text: text.lower())\n","    \n","    tokenized_train = Dataset.from_pandas(train_df, split='train').map(preprocess_function, batched=True)\n","    tokenized_test  = Dataset.from_pandas( test_df, split='test' ).map(preprocess_function, batched=True)\n","        \n","    # Train and evaluate\n","    lora_model = get_peft_model(\n","        prepare_model_for_int8_training(model), \n","        LoraConfig(\n","            r=64,\n","            lora_alpha=32,\n","            lora_dropout=0.1,\n","            task_type=TaskType.SEQ_CLS,\n","            target_modules='all-linear'\n","        )\n","    )\n","    trainer = Trainer(\n","        model = lora_model,\n","        tokenizer = tokenizer,\n","        train_dataset = tokenized_train, \n","        eval_dataset = tokenized_test,\n","        compute_metrics = compute_metrics,\n","        data_collator = DataCollatorWithPadding(tokenizer=tokenizer),\n","        args = TrainingArguments(\n","            output_dir = './checkpoints/',\n","            report_to = 'none',\n","            evaluation_strategy = \"epoch\",\n","            save_strategy = \"epoch\",\n","            save_total_limit = 1,\n","            load_best_model_at_end = True,\n","            **train_args\n","        )\n","    )\n","    \n","    display(trainer.evaluate())\n","    train_output = trainer.train()\n","    display(trainer.evaluate())\n","    \n","    predictions  = trainer.predict(tokenized_test)\n","    \n","    y_test, y_pred = test_df.label, predictions.predictions.argmax(1)\n","    \n","    clf_report  = classification_report(y_test, y_pred, target_names=label_names, digits=5)\n","    conf_matrix = confusion_matrix(y_test, y_pred)\n","    \n","    if save_model:\n","        trainer.save_model(f\"/kaggle/working/model_{model_name.replace('/', '-')}/\")\n","        \n","    !rm -r './checkpoints/'\n","    return clf_report, conf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 2e-5,\n","        'weight_decay': 0.01,\n","        'per_device_train_batch_size': 4,\n","        'logging_steps': 10,\n","    },\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4713710,"sourceId":8003983,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
