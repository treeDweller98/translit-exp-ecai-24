{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-20T18:01:11.986042Z","iopub.status.busy":"2024-04-20T18:01:11.985708Z","iopub.status.idle":"2024-04-20T18:01:26.447824Z","shell.execute_reply":"2024-04-20T18:01:26.446726Z","shell.execute_reply.started":"2024-04-20T18:01:11.986015Z"},"trusted":true},"outputs":[],"source":["!pip install evaluate -q"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:26.450250Z","iopub.status.busy":"2024-04-20T18:01:26.449954Z","iopub.status.idle":"2024-04-20T18:01:46.968401Z","shell.execute_reply":"2024-04-20T18:01:46.967547Z","shell.execute_reply.started":"2024-04-20T18:01:26.450223Z"},"trusted":true},"outputs":[],"source":["import re, regex, os, sys, warnings, random, gc, logging\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import train_test_split\n","\n","import evaluate\n","from datasets import Dataset\n","\n","import transformers\n","from transformers import (\n","    LineByLineTextDataset,\n","    DataCollatorWithPadding, DataCollatorForLanguageModeling,\n","    TrainingArguments, Trainer,\n","    AutoTokenizer, AutoModel, AutoModelForMaskedLM, AutoModelForSequenceClassification,\n",")\n","\n","\n","SEED = 42\n","transformers.set_seed(SEED)\n","warnings.filterwarnings('ignore')\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","logging.getLogger().setLevel(logging.WARNING)\n","\n","INPUT_PATH = '/kaggle/input/translit-datasets/'"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:46.970217Z","iopub.status.busy":"2024-04-20T18:01:46.969611Z","iopub.status.idle":"2024-04-20T18:01:46.980105Z","shell.execute_reply":"2024-04-20T18:01:46.979190Z","shell.execute_reply.started":"2024-04-20T18:01:46.970189Z"},"trusted":true},"outputs":[],"source":["# Insert appropriate dataset reading code from dataset_readers.py\n","\n","def read_dataset():\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:46.982347Z","iopub.status.busy":"2024-04-20T18:01:46.982074Z","iopub.status.idle":"2024-04-20T18:01:47.667157Z","shell.execute_reply":"2024-04-20T18:01:47.666163Z","shell.execute_reply.started":"2024-04-20T18:01:46.982323Z"},"trusted":true},"outputs":[],"source":["train_df, test_df, label_names, dataset_name, text_col = read_dataset()\n","\n","train_df.label = train_df.label.cat.codes\n","test_df.label = test_df.label.cat.codes\n","\n","class_weights = dict(enumerate(\n","    compute_class_weight(\n","        class_weight=\"balanced\", \n","        classes=np.unique(train_df['label']), \n","        y=train_df['label']\n","    )\n","))\n","\n","pd.set_option('max_colwidth', 200)\n","display(train_df.head())\n","display(test_df.head())\n","\n","print(f'{len(train_df)=}, {len(test_df)=}')\n","print(label_names)\n","\n","plt.figure(figsize=(6,2))\n","plt.bar(x=label_names, height=np.bincount(train_df['label']))"]},{"cell_type":"markdown","metadata":{},"source":["# Trainer code"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:47.668714Z","iopub.status.busy":"2024-04-20T18:01:47.668399Z","iopub.status.idle":"2024-04-20T18:01:47.710399Z","shell.execute_reply":"2024-04-20T18:01:47.709349Z","shell.execute_reply.started":"2024-04-20T18:01:47.668687Z"},"trusted":true},"outputs":[],"source":["EPOCHS = 6\n","\n","def train_maskedlm(model_name, train_df, test_df):\n","    # Prepare data for training\n","    data = pd.concat([train_df, test_df])\n","    data['sentence'] = data['sentence'].apply(lambda x: x.replace('\\n',''))\n","    \n","    text  = '\\n'.join(data.sentence.tolist())\n","    with open('text.txt','w') as f:\n","        f.write(text)\n","        \n","    # Train model\n","    model = AutoModelForMaskedLM.from_pretrained(model_name)\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    tokenizer.save_pretrained(f\"/kaggle/working/model_{model_name.replace('/', '-')}/tokenizer/\")\n","    \n","    trainer = Trainer(\n","        model=model,\n","        train_dataset = LineByLineTextDataset(\n","            tokenizer=tokenizer,\n","            file_path=\"text.txt\",\n","            block_size=256\n","        ),\n","        eval_dataset = LineByLineTextDataset(\n","            tokenizer=tokenizer,\n","            file_path=\"text.txt\",\n","            block_size=256\n","        ),\n","        data_collator = DataCollatorForLanguageModeling(\n","            tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n","        ),\n","        args = TrainingArguments(\n","            output_dir=f\"./checkpoints/\",\n","            overwrite_output_dir=True,\n","            num_train_epochs=EPOCHS,\n","            per_device_train_batch_size=8,\n","            per_device_eval_batch_size=8,\n","            evaluation_strategy= 'steps',\n","            save_total_limit=2,\n","            eval_steps=200,\n","            save_steps=400,\n","            metric_for_best_model='eval_loss',\n","            greater_is_better=False,\n","            load_best_model_at_end =True,\n","            prediction_loss_only=True,\n","            report_to = \"none\"\n","        ),\n","    )\n","    \n","    trainer.train()\n","    \n","    model_path = f\"/kaggle/working/model_{model_name.replace('/', '-')}/maskedlm/\"\n","    trainer.save_model(model_path)\n","    !rm -r './checkpoints/'\n","    \n","    return model_path\n","    \n","    \n","def train_model( \n","    model_name: str,\n","    train_args: dict,\n","    seed: int = SEED,\n","    train_df: pd.DataFrame = train_df, \n","    test_df: pd.DataFrame = test_df, \n","    label_names: list[str] = label_names,\n","    save_model: bool = True,\n","    train_maskedLM = False\n","):\n","    # Setup\n","    transformers.set_seed(seed)\n","    n_labels = len(label_names)\n","    id2label = {i:name for i, name in enumerate(label_names)}\n","    label2id = {name:i for i, name in enumerate(label_names)}\n","    \n","    if train_maskedLM:\n","        # trains and saves a model at path model_name\n","        model_name = train_maskedlm(model_name, train_df, test_df)\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_name, num_labels=n_labels, id2label=id2label, label2id=label2id,\n","    )\n","    \n","    metric = evaluate.load(\"f1\")\n","    def compute_metrics(eval_pred):\n","        logits, labels = eval_pred\n","        predictions = np.argmax(logits, axis=1)\n","        return metric.compute(predictions=predictions, references=labels, average='weighted')  \n","    \n","    training_args = TrainingArguments(\n","        output_dir='./checkpoints/',\n","        evaluation_strategy=\"epoch\",\n","        report_to='none',\n","        save_strategy=\"epoch\",\n","        save_total_limit=1,\n","        load_best_model_at_end=True,\n","        **train_args\n","    )\n","    \n","    # Make Dataset and tokenize    \n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    def preprocess_function(example):\n","        return tokenizer(example[\"sentence\"])\n","\n","    train_df.sentence = train_df.sentence.apply(lambda text: text.lower())\n","    test_df.sentence  =  test_df.sentence.apply(lambda text: text.lower())\n","    \n","    tokenized_train = Dataset.from_pandas(train_df, split='train').map(preprocess_function, batched=True)\n","    tokenized_test  = Dataset.from_pandas( test_df, split='test' ).map(preprocess_function, batched=True)\n","    \n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","    \n","    # Train and evaluate\n","    trainer = Trainer(\n","        model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test,\n","        tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics,\n","    )\n","\n","    trainer.evaluate()\n","    train_output = trainer.train()\n","    trainer.evaluate()\n","    predictions  = trainer.predict(tokenized_test)\n","    \n","    y_test, y_pred = test_df.label, predictions.predictions.argmax(1)\n","    \n","    clf_report  = classification_report(y_test, y_pred, target_names=label_names, digits=5)\n","    conf_matrix = confusion_matrix(y_test, y_pred)\n","    \n","    if save_model:\n","        trainer.save_model(f\"/kaggle/working/model_{model_name.replace('/', '-')}/\")\n","        \n","    !rm -r './checkpoints/'\n","    return clf_report, conf_matrix"]},{"cell_type":"markdown","metadata":{},"source":["# Mixed Language Models"]},{"cell_type":"markdown","metadata":{},"source":["## BERT-base-uncased"]},{"cell_type":"code","execution_count":6,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T18:01:47.711828Z","iopub.status.busy":"2024-04-20T18:01:47.711508Z","iopub.status.idle":"2024-04-20T18:01:47.725948Z","shell.execute_reply":"2024-04-20T18:01:47.725001Z","shell.execute_reply.started":"2024-04-20T18:01:47.711802Z"},"trusted":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    \"google-bert/bert-base-uncased\",\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6\n","    }\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:47.727516Z","iopub.status.busy":"2024-04-20T18:01:47.727210Z","iopub.status.idle":"2024-04-20T18:01:47.735743Z","shell.execute_reply":"2024-04-20T18:01:47.734708Z","shell.execute_reply.started":"2024-04-20T18:01:47.727492Z"},"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["## BanglishBERT"]},{"cell_type":"code","execution_count":8,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T18:01:47.737291Z","iopub.status.busy":"2024-04-20T18:01:47.736966Z","iopub.status.idle":"2024-04-20T18:01:47.745239Z","shell.execute_reply":"2024-04-20T18:01:47.744467Z","shell.execute_reply.started":"2024-04-20T18:01:47.737259Z"},"trusted":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    'csebuetnlp/banglishbert',\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6\n","    }\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:47.747256Z","iopub.status.busy":"2024-04-20T18:01:47.746630Z","iopub.status.idle":"2024-04-20T18:01:47.755421Z","shell.execute_reply":"2024-04-20T18:01:47.754436Z","shell.execute_reply.started":"2024-04-20T18:01:47.747224Z"},"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["## HingBERT"]},{"cell_type":"code","execution_count":10,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T18:01:47.760719Z","iopub.status.busy":"2024-04-20T18:01:47.760456Z","iopub.status.idle":"2024-04-20T18:01:47.765361Z","shell.execute_reply":"2024-04-20T18:01:47.764537Z","shell.execute_reply.started":"2024-04-20T18:01:47.760696Z"},"trusted":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    'l3cube-pune/hing-bert',\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6\n","    }\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:47.767291Z","iopub.status.busy":"2024-04-20T18:01:47.766615Z","iopub.status.idle":"2024-04-20T18:01:47.775315Z","shell.execute_reply":"2024-04-20T18:01:47.774533Z","shell.execute_reply.started":"2024-04-20T18:01:47.767260Z"},"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["## DarijaBERT"]},{"cell_type":"code","execution_count":12,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T18:01:47.776605Z","iopub.status.busy":"2024-04-20T18:01:47.776381Z","iopub.status.idle":"2024-04-20T18:01:47.785424Z","shell.execute_reply":"2024-04-20T18:01:47.784601Z","shell.execute_reply.started":"2024-04-20T18:01:47.776585Z"},"trusted":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    \"SI2M-Lab/DarijaBERT-arabizi\",\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6\n","    }\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:47.786976Z","iopub.status.busy":"2024-04-20T18:01:47.786615Z","iopub.status.idle":"2024-04-20T18:01:47.795778Z","shell.execute_reply":"2024-04-20T18:01:47.794828Z","shell.execute_reply.started":"2024-04-20T18:01:47.786942Z"},"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["# Multilingual models"]},{"cell_type":"markdown","metadata":{},"source":["## XLM-RoBERTa"]},{"cell_type":"code","execution_count":14,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T18:01:47.797139Z","iopub.status.busy":"2024-04-20T18:01:47.796889Z","iopub.status.idle":"2024-04-20T18:01:47.805956Z","shell.execute_reply":"2024-04-20T18:01:47.805009Z","shell.execute_reply.started":"2024-04-20T18:01:47.797117Z"},"trusted":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    'xlm-roberta-base',\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6\n","    },\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:47.807334Z","iopub.status.busy":"2024-04-20T18:01:47.807042Z","iopub.status.idle":"2024-04-20T18:01:47.816156Z","shell.execute_reply":"2024-04-20T18:01:47.815199Z","shell.execute_reply.started":"2024-04-20T18:01:47.807301Z"},"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["## mDeBERTa"]},{"cell_type":"code","execution_count":16,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T18:01:47.817532Z","iopub.status.busy":"2024-04-20T18:01:47.817232Z","iopub.status.idle":"2024-04-20T18:01:47.825794Z","shell.execute_reply":"2024-04-20T18:01:47.824869Z","shell.execute_reply.started":"2024-04-20T18:01:47.817508Z"},"trusted":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    'microsoft/mdeberta-v3-base',\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6\n","    }\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:47.827230Z","iopub.status.busy":"2024-04-20T18:01:47.826922Z","iopub.status.idle":"2024-04-20T18:01:47.836327Z","shell.execute_reply":"2024-04-20T18:01:47.835471Z","shell.execute_reply.started":"2024-04-20T18:01:47.827198Z"},"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["## mBERT"]},{"cell_type":"code","execution_count":18,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T18:01:47.837728Z","iopub.status.busy":"2024-04-20T18:01:47.837413Z","iopub.status.idle":"2024-04-20T18:01:47.846613Z","shell.execute_reply":"2024-04-20T18:01:47.845614Z","shell.execute_reply.started":"2024-04-20T18:01:47.837704Z"},"trusted":true},"outputs":[],"source":["clf_report, acconf_matrix = train_model(\n","    'bert-base-multilingual-cased',\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6\n","    }\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:47.848111Z","iopub.status.busy":"2024-04-20T18:01:47.847770Z","iopub.status.idle":"2024-04-20T18:01:47.860670Z","shell.execute_reply":"2024-04-20T18:01:47.859806Z","shell.execute_reply.started":"2024-04-20T18:01:47.848081Z"},"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["# Character Models"]},{"cell_type":"markdown","metadata":{},"source":["## CharBERT"]},{"cell_type":"code","execution_count":20,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T18:01:47.862221Z","iopub.status.busy":"2024-04-20T18:01:47.861862Z","iopub.status.idle":"2024-04-20T18:01:47.871945Z","shell.execute_reply":"2024-04-20T18:01:47.871053Z","shell.execute_reply.started":"2024-04-20T18:01:47.862189Z"},"trusted":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    'imvladikon/charbert-bert-wiki',\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6\n","    }\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T18:01:47.873322Z","iopub.status.busy":"2024-04-20T18:01:47.873055Z","iopub.status.idle":"2024-04-20T18:01:47.881051Z","shell.execute_reply":"2024-04-20T18:01:47.880195Z","shell.execute_reply.started":"2024-04-20T18:01:47.873299Z"},"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["## CharBERT-RoBERTa"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-04-20T18:01:47.883913Z","iopub.status.busy":"2024-04-20T18:01:47.882223Z"},"trusted":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    'imvladikon/charbert-roberta-wiki',\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]},{"cell_type":"markdown","metadata":{},"source":["## CANINE"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["clf_report, conf_matrix = train_model(\n","    'google/canine-c',\n","    train_args = {\n","        'num_train_epochs': EPOCHS,\n","        'learning_rate': 5e-6,\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(clf_report)\n","ConfusionMatrixDisplay(conf_matrix, display_labels=label_names).plot()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4713710,"sourceId":8154827,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
